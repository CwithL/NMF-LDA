{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vp vp processed vp vp virus assembly p p regio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>december novel coronavirus sarscov identified ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outbreak infectious disease always one importa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metagenomic sequencing allows u directly obtai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>since emergence december novel coronavirus sar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TextBody\n",
       "0  vp vp processed vp vp virus assembly p p regio...\n",
       "1  december novel coronavirus sarscov identified ...\n",
       "2  outbreak infectious disease always one importa...\n",
       "3  metagenomic sequencing allows u directly obtai...\n",
       "4  since emergence december novel coronavirus sar..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('..')\n",
    "# 将数据读入文件\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\Asus\\\\nmf\\\\'\n",
    "file_name = 'topic_cleaned_data.csv'\n",
    "\n",
    "papers = pd.read_csv(file_path + file_name,nrows = 6000)\n",
    "papers = papers.drop(columns=['Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','Title','Abstract','Source','PubDate','Journal','DOI','FirstAuthor','OtherAuthors','Keywords','Languages'],axis=1)\n",
    "\n",
    "# 输出显示前5个数据\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1691    december world health organization china count...\n",
       "4944    variety virus large growing picornaviridae fam...\n",
       "5173    member picornaviridae family positivestrand vi...\n",
       "5551    introduction respiratory infection significant...\n",
       "3148    hepatitis c virus hcv affect million people wo...\n",
       "Name: TextBody, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "papers= papers['TextBody']\n",
    "\n",
    "papers = papers.sample(5000)\n",
    "\n",
    "# 打印第一行\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['december', 'world', 'health', 'organization', 'china', 'country', 'office', 'informed', 'case', 'pneumonia', 'unknown', 'etiology', 'unknown', 'cause', 'detected', 'wuhan', 'city', 'hubei', 'province', 'china', 'reported', 'novel', 'coronavirus', 'ncov', 'named', 'severe', 'acute', 'respiratory', 'syndrome', 'coronavirus']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "# 加载正则表达式库\n",
    "import re\n",
    "\n",
    "# 删除标点符号\n",
    "papers['paper_text_processed'] = papers.map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "#将标题转换为小写\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# 打印第一排\n",
    "papers['paper_text_processed'].head()\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True 删除标点符号\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立二联体和三联体模型\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# 这是把一个句子打成三字母或两字母组合的更快方法\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "D:\\python\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "# NLTK 停用词\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# 三元组\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# 初始化空间'en'模型，只保留标记组件(为了效率)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# 只保留n, v ,adj,  adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# 创建语料库\n",
    "texts = data_lemmatized\n",
    "# 文档频率\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创捷LDA模型\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# 计算连贯性得分\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA#LDA\n",
    "coherence_scores=[0.254,0.337,0.358,0.377,0.386,0.408,0.412,0.414,0.415,0.419]\n",
    "# 结果展示\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.plot(\n",
    "    coherence_scores,\n",
    "    linewidth=3,\n",
    "    color='#000000'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"主题数目\", fontsize=14)\n",
    "plt.ylabel(\"连贯性评分\", fontsize=14)\n",
    "plt.title('根据主题数的连贯性评分 - 最好的主题数: 10', fontsize=18)\n",
    "plt.xticks(np.arange(1, max(topic_nums) + 1, 1), fontsize=12)\n",
    "\n",
    "\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "file_name = 'c_score'\n",
    "\n",
    "fig.savefig(\n",
    "    file_path + file_name + '.png',\n",
    "    dpi=fig.dpi,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,   #导入待训练语料\n",
    "                                           id2word=id2word,  #设置构建模型的字典，决定了词汇数量\n",
    "                                           num_topics=10,  #需要提取的潜在主题数                                     \n",
    "                                           random_state=100, #随机化设置\n",
    "                                           chunksize=5000,  #文件块大小\n",
    "                                           passes=10,    #训练轮次数\n",
    "                                           alpha=0.01,   #决定文档主题狄利克雷先验分布的超参数\n",
    "                                           eta=0.9)     #决定主题词汇狄利克雷先验分布的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import pickle \n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
